{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3420079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Pandas für Datenmanipulation\n",
    "import dask.dataframe as dd # Dask für große Datenmengen\n",
    "import zipfile\n",
    "import requests # HTTP-Anfragen\n",
    "import shutil # Löschen von Ordnern\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed # Multithreading-Unterstützung\n",
    "import os\n",
    "from tqdm import tqdm # Fortschrittsbalken\n",
    "import time\n",
    "import glob # Dateimustererkennung\n",
    "import re\n",
    "import xml.etree.ElementTree as ET # Der native XML-Parser\n",
    "import pyarrow as pa # \n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e61ef",
   "metadata": {},
   "source": [
    "Citibike-Daten verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0726d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generierte Download-Muster: ['^JC-2014\\\\d{2}-.*\\\\.zip$', '^2014-.*\\\\.zip$', '^2014\\\\d{2}-.*\\\\.zip$']\n"
     ]
    }
   ],
   "source": [
    "# Liste der Jahre/Muster, die Sie herunterladen möchten. Mögliche Formate:\n",
    "# 1. \"*\"\" - Alle Jahre \n",
    "# 2. \"YYYY-YYYY\" - Jahr-Bereich (z.B. [2015-2017])\n",
    "# 3. [2016, 2017, 2018] - Spezifische Jahre\n",
    "TARGET_YEARS = [2014]\n",
    "\n",
    "# --- Generierung der Download-Muster basierend auf TARGET_YEARS ---\n",
    "def generate_download_patterns(target_input):\n",
    "    \"\"\"\n",
    "    Generiert die regulären Ausdrücke für die Zielfilterung basierend auf den drei definierten Formaten.\n",
    "    \"\"\"\n",
    "    years = []\n",
    "    current_year = datetime.now().year\n",
    "    first_data_year = 2013 # Startjahr der CitiBike-Daten\n",
    "\n",
    "    # --- Verarbeitung der Eingabe ---\n",
    "    if isinstance(target_input, list):\n",
    "        # Format: [2016, 2017]\n",
    "        years = target_input\n",
    "    \n",
    "    elif isinstance(target_input, str) and target_input == \"*\":\n",
    "        # Format: \"*\"\" -> Alle Jahre\n",
    "        years = list(range(first_data_year, current_year + 1))\n",
    "        \n",
    "    elif isinstance(target_input, str) and re.match(r\"^\\d{4}-\\d{4}$\", target_input):\n",
    "        # Format: \"YYYY-YYYY\" -> Jahr-Bereich\n",
    "        try:\n",
    "            start_year, end_year = map(int, target_input.split('-'))\n",
    "            if start_year > end_year:\n",
    "                raise ValueError(\"Startjahr muss kleiner oder gleich dem Endjahr sein.\")\n",
    "            years = list(range(start_year, end_year + 1))\n",
    "        except ValueError as e:\n",
    "            print(f\"FEHLER beim Parsen des Jahresbereichs: {e}\")\n",
    "            return []\n",
    "            \n",
    "    else:\n",
    "        # Ungültige Eingabe\n",
    "        print(\"FEHLER: Ungültiges TARGET_YEARS Format. Erlaubt sind: [*], [YYYY-YYYY] oder eine Liste von Jahren [2016, 2017].\")\n",
    "        return []\n",
    "\n",
    "    # Optional: Ungültige/Zukünftige Jahre filtern\n",
    "    years = [y for y in years if first_data_year <= y <= current_year]\n",
    "    \n",
    "    if not years:\n",
    "        print(\"Keine gültigen Jahre gefunden, die heruntergeladen werden können.\")\n",
    "        return []\n",
    "\n",
    "    # --- Generierung der Regex-Muster (wie zuvor, aber auf die gefilterten Jahre angewandt) ---\n",
    "    patterns = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_str = str(year)\n",
    "        # Muster für monatliche Dateien (202301-...) und ältere/JC-Formate\n",
    "        patterns.append(rf\"^{year_str}\\d{{2}}-.*\\.zip$\")\n",
    "        patterns.append(rf\"^{year_str}-.*\\.zip$\")\n",
    "        patterns.append(rf\"^JC-{year_str}\\d{{2}}-.*\\.zip$\")\n",
    "        \n",
    "    # Entferne Duplikate\n",
    "    return list(set(patterns))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    YEAR_PATTERNS = generate_download_patterns(TARGET_YEARS)\n",
    "    print(\"Generierte Download-Muster:\", YEAR_PATTERNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23176b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Crawling der S3-Seite: https://s3.amazonaws.com/tripdata/\n",
      "HTTP-Statuscode der Antwort: 200\n",
      "Gefundene <Key>-Elemente im XML: 158\n",
      "Gefundene relevante Datei: 2014-citibike-tripdata.zip\n",
      "Insgesamt 1 Dateien zum Herunterladen gefunden.\n",
      "  > Überspringe: 2014-citibike-tripdata.zip (existiert bereits)\n",
      "\n",
      "--- Download abgeschlossen ---\n"
     ]
    }
   ],
   "source": [
    "# --- Konfiguration ---\n",
    "BASE_URL = \"https://s3.amazonaws.com/tripdata/\"  # S3-Indexseite mit den Citibike-Daten\n",
    "RAW_DATA_DIR = \"../data/raw/citibike\" \n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "    os.makedirs(RAW_DATA_DIR)\n",
    "    print(f\"Lokaler Download-Ordner erstellt: {RAW_DATA_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Hauptlogik ---\n",
    "def find_and_download_files():\n",
    "    \"\"\"Crawlt die S3-Seite und lädt alle relevanten ZIP-Dateien herunter.\"\"\"\n",
    "    print(f\"Starte Crawling der S3-Seite: {BASE_URL}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Führe eine GET-Anfrage an die S3-Indexseite durch\n",
    "        response = requests.get(BASE_URL)\n",
    "        print(f\"HTTP-Statuscode der Antwort: {response.status_code}\")\n",
    "        response.raise_for_status() # Löst Fehler bei ungültigem Status (4xx, 5xx) aus\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"FEHLER beim Zugriff auf die S3-URL: {e}\")\n",
    "        return\n",
    "\n",
    "    download_list = []\n",
    "\n",
    "    # # Parse den XML-Inhalt hat weder mit html.parser noch mit lxml-xml funktioniert\n",
    "    # # soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "    \n",
    "\n",
    "    # Alternative Methode: Verwende den nativen XML-Parser\n",
    "    try:\n",
    "        # 1. Parse den XML-Inhalt mit ElementTree\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # 2. S3 verwendet Namespaces; wir müssen den Namespace aus dem Root-Tag extrahieren\n",
    "        # Beispiel: {http://s3.amazonaws.com/doc/2006-03-01/}\n",
    "        namespace = re.match(r'\\{.*\\}', root.tag).group(0)\n",
    "        \n",
    "        # 3. Finde alle 'Key'-Tags innerhalb des S3-Listings\n",
    "        # Der Tag-Name muss mit dem extrahierten Namespace verwendet werden\n",
    "        keys = root.findall(f'.//{namespace}Key')\n",
    "\n",
    "        print(f\"Gefundene <Key>-Elemente im XML: {len(keys)}\")\n",
    "        \n",
    "        # 4. Filterung und Sammeln der Dateinamen\n",
    "        for key in keys:\n",
    "            filename = key.text\n",
    "            \n",
    "            if not filename or not filename.endswith(\".zip\"):\n",
    "                continue\n",
    "                \n",
    "            is_relevant = False\n",
    "            for pattern in YEAR_PATTERNS:\n",
    "                if any(re.match(pattern.replace('*', '.*'), filename) for pattern in YEAR_PATTERNS):\n",
    "                # if re.match(pattern, filename): \n",
    "                    is_relevant = True\n",
    "                    break\n",
    "            \n",
    "            if is_relevant:\n",
    "                print(f\"Gefundene relevante Datei: {filename}\")\n",
    "                download_list.append(filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"KRITISCHER FEHLER beim Parsen der S3-Antwort: {e}\")\n",
    "        return\n",
    "    if not download_list:\n",
    "        print(\"Keine Dateien gefunden, die den Suchmustern entsprechen. Prüfen Sie die YEAR_PATTERNS.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Insgesamt {len(download_list)} Dateien zum Herunterladen gefunden.\")\n",
    "    \n",
    "    # Starte den Download-Prozess\n",
    "    for filename in download_list:\n",
    "        file_url = BASE_URL + filename\n",
    "        local_path = os.path.join(RAW_DATA_DIR, filename)\n",
    "\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"  > Überspringe: {filename} (existiert bereits)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  > Downloade: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Streaming-Download, um den Speicher nicht zu überlasten\n",
    "            with requests.get(file_url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                \n",
    "                with open(local_path, 'wb') as f:\n",
    "                    # Fortschrittsbalken mit tqdm\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename) as t:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                            t.update(len(chunk))\n",
    "            \n",
    "            # Kurze Pause, um den Server nicht zu überlasten\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  FEHLER beim Download von {filename}: {e}\")\n",
    "            time.sleep(5) # Längere Pause bei Fehlern\n",
    "            continue\n",
    "\n",
    "    print(\"\\n--- Download abgeschlossen ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82040905",
   "metadata": {},
   "source": [
    "2019 führt Citibike E-Bikes ein\n",
    "\n",
    "Spaltennamen ändern sich:\n",
    "2014-2019: tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
    "2020-2025: ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa04db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Verarbeitung von 155 ZIP-Dateien...\n",
      "\n",
      "--- Phase 1: Entpacken der ZIPs ---\n",
      "\n",
      "488 CSV-Dateien gefunden und bereit zur Parallelverarbeitung.\n",
      "\n",
      "--- Phase 2: Parallelverarbeitung der CSVs ---\n",
      "\n",
      "--- Phase 3: Finale Konsolidierung ---\n",
      "Füge 488 temporäre Parquet-Dateien zusammen...\n",
      "Lese 488 Parquet-Dateien (lazy)...\n",
      "Schreibe die kombinierte Tabelle nach ../data/processed/citibike\\citibike_all_years_combined.parquet...\n",
      "\n",
      "--- ERFOLG ---\n",
      "Gesamtdatensatz gespeichert unter: ../data/processed/citibike\\citibike_all_years_combined.parquet\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.6 MiB for an array with shape (4, 888085) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 420\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;66;03m# 8. Aufräumen (optional)\u001b[39;00m\n\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# Entfernen Sie die temporären Dateien, um Speicherplatz zu sparen\u001b[39;00m\n\u001b[32m    415\u001b[39m     \u001b[38;5;66;03m# for f in final_parquet_files: os.remove(f)\u001b[39;00m\n\u001b[32m    416\u001b[39m     \u001b[38;5;66;03m# for f in all_csv_files: os.remove(f)\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    419\u001b[39m     \u001b[38;5;66;03m# Achtung: Die Ausführung dieses Skripts kann bei allen Jahren Stunden dauern!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[43morchestrate_data_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 411\u001b[39m, in \u001b[36morchestrate_data_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- ERFOLG ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    410\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGesamtdatensatz gespeichert unter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINAL_PARQUET_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinale Zeilenanzahl: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mddf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\dask\\base.py:378\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    355\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    356\u001b[39m \n\u001b[32m    357\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    376\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\dask\\base.py:686\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     expr = expr.optimize()\n\u001b[32m    684\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:82\u001b[39m, in \u001b[36mParquetFunctionWrapper.__call__\u001b[39m\u001b[34m(self, part)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(part, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     80\u001b[39m     part = [part]\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_parquet_part\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Temporary workaround for HLG serialization bug\u001b[39;49;00m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (see: https://github.com/dask/dask/issues/8581)\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:184\u001b[39m, in \u001b[36mread_parquet_part\u001b[39m\u001b[34m(fs, engine, meta, part, columns, index, kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(part) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m part[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_multi_support(engine):\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# Part kwargs expected\u001b[39;00m\n\u001b[32m    182\u001b[39m     func = engine.read_partition\n\u001b[32m    183\u001b[39m     dfs = [\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtoolz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m (rg, kw) \u001b[38;5;129;01min\u001b[39;00m part\n\u001b[32m    192\u001b[39m     ]\n\u001b[32m    193\u001b[39m     df = concat(dfs, axis=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dfs[\u001b[32m0\u001b[39m]\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;66;03m# list of parts at once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:632\u001b[39m, in \u001b[36mArrowDatasetEngine.read_partition\u001b[39m\u001b[34m(cls, fs, pieces, columns, index, dtype_backend, categories, partitions, filters, schema, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m     index = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    631\u001b[39m     columns_and_parts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(columns_and_parts) - \u001b[38;5;28mset\u001b[39m(df.index.names))\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns_and_parts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index:\n\u001b[32m    635\u001b[39m     df = df.set_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 13.6 MiB for an array with shape (4, 888085) and data type float32"
     ]
    }
   ],
   "source": [
    "UNZIPPED_DIR = \"../data/unzipped/citibike\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed/citibike\"\n",
    "FINAL_PARQUET_PATH = os.path.join(PROCESSED_DATA_DIR, 'citibike_all_years_combined.parquet')\n",
    "\n",
    "# Daten-Typen-Optimierung um Speicher (RAM) zu sparen\n",
    "# WICHTIG: Datums- und Zeit-Spalten werden NICHT in der DTYPE_MAP auf datetime\n",
    "# gesetzt, da dies in read_csv zu Problemen führt. Sie werden separat konvertiert.\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    # --- Geo-Koordinaten (Alt & Neu) ---\n",
    "    'start_lat': 'float32',\n",
    "    'start_lng': 'float32',\n",
    "    'end_lat': 'float32',\n",
    "    'end_lng': 'float32',\n",
    "    'start station latitude': 'float32',\n",
    "    'start station longitude': 'float32',\n",
    "    'end station latitude': 'float32',\n",
    "    'end station longitude': 'float32',\n",
    "    'Start Station Latitude': 'float32',\n",
    "    'Start Station Longitude': 'float32',\n",
    "    'End Station Latitude': 'float32',\n",
    "    'End Station Longitude': 'float32',\n",
    "    \n",
    "    # --- IDs ---\n",
    "    'ride_id': 'string',\n",
    "    'bikeid': 'int32',         # Alte bikeid ist oft nur eine Zahl\n",
    "    'Bike ID': 'int32',\n",
    "    'start_station_id': 'string', # IDs sollten in Int umgewandelt werden\n",
    "    'end_station_id': 'string',\n",
    "    'start station id': 'string', \n",
    "    'end station id': 'string',\n",
    "    'Start Station Id': 'string',\n",
    "    'End Station Id': 'string',\n",
    "    \n",
    "    # --- Typen und Kategorien ---\n",
    "    'rideable_type': 'category',\n",
    "    'usertype': 'category',\n",
    "    'User Type': 'category',\n",
    "    'member_casual': 'category',\n",
    "    'gender': 'category',\n",
    "    'Gender': 'category',\n",
    "    \n",
    "    # --- Numerische Werte ---\n",
    "    'tripduration': 'int32',\n",
    "    'Trip Duration': 'int32',\n",
    "    'birth year': 'float16', # sollte in Int umgewandelt werden\n",
    "    'Birth Year': 'float16', # sollte in Int umgewandelt werden\n",
    "    \n",
    "    # --- Stationsnamen  ---\n",
    "    'start station name': 'string', \n",
    "    'end station name': 'string', \n",
    "    'start_station_name': 'string',\n",
    "    'end_station_name': 'string',\n",
    "    'Start Station Name': 'string',\n",
    "    'End Station Name': 'string',\n",
    "    }\n",
    "\n",
    "\n",
    "# 2014-2018: tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
    "# 2090-2025: ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
    "\n",
    "\n",
    "# def unzip_file(zip_path, output_dir):\n",
    "#     \"\"\"\n",
    "#     Entpackt eine ZIP-Datei (auch verschachtelte ZIPs) und prüft, \n",
    "#     ob die Entpackung bereits erfolgt ist.\n",
    "#     \"\"\"\n",
    "#     zip_filename = os.path.basename(zip_path)\n",
    "    \n",
    "#     try:\n",
    "#         with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "#             members_to_extract = zf.namelist()\n",
    "#             csv_members = [m for m in members_to_extract if m.endswith('.csv')]\n",
    "            \n",
    "#             if csv_members:\n",
    "#                 # NEUE PRÜFUNG: Wenn die erste erwartete CSV existiert, überspringen\n",
    "#                 first_csv_name = os.path.basename(csv_members[0])\n",
    "#                 expected_csv_path = os.path.join(output_dir, first_csv_name)\n",
    "                \n",
    "#                 if os.path.exists(expected_csv_path):\n",
    "#                     return f\"Überspringe: {zip_filename} (CSV-Datei '{first_csv_name}' existiert bereits).\"\n",
    "            \n",
    "#             # Alle Mitglieder entpacken\n",
    "#             for member in members_to_extract:\n",
    "                \n",
    "#                 # Handling verschachtelter ZIPs\n",
    "#                 if member.endswith('.zip'):\n",
    "#                     inner_zip_path = zf.extract(member, path=output_dir)\n",
    "#                     with zipfile.ZipFile(inner_zip_path, 'r') as inner_zf:\n",
    "#                         inner_zf.extractall(output_dir)\n",
    "#                     os.remove(inner_zip_path) # Temporäre innere ZIP-Datei löschen\n",
    "                    \n",
    "#                 # Handling von CSV-Dateien\n",
    "#                 elif member.endswith('.csv'):\n",
    "#                     zf.extract(member, path=output_dir)\n",
    "                    \n",
    "#         return f\"Erfolg: {zip_filename} entpackt.\"\n",
    "#     except Exception as e:\n",
    "#         return f\"FEHLER: {zip_filename} konnte nicht entpackt werden. {e}\"\n",
    "\n",
    "\n",
    "def unzip_file(zip_path, output_dir):\n",
    "    \"\"\"\n",
    "    Entpackt eine ZIP-Datei und verarbeitet alle darin enthaltenen Strukturen: \n",
    "    verschachtelte Ordner, verschachtelte ZIPs und direkte CSVs.\n",
    "    \"\"\"\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    \n",
    "    # 1. Prüfe auf bereits entpackte Daten (der Skip-Check bleibt)\n",
    "    # Da die Namenskonventionen variieren, ist dieser Check komplexer. \n",
    "    # Für Einfachheit überspringen wir den Check für das Entpacken und lassen die \n",
    "    # CSV-Verarbeitung (Phase 2) die Duplikate handhaben. Wir führen den Check nur \n",
    "    # für die temporären Verzeichnisse durch, die wir erstellen.\n",
    "    \n",
    "    temp_extract_dir = os.path.join(output_dir, f\"temp_extract_{zip_filename.replace('.zip', '')}\")\n",
    "    os.makedirs(temp_extract_dir, exist_ok=True) # Temporäres Verzeichnis für das Entpacken\n",
    "\n",
    "    try:\n",
    "        # 2. Entpacke das Hauptarchiv in ein temporäres Verzeichnis\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(temp_extract_dir)\n",
    "\n",
    "        # 3. Durchsuche das temporäre Verzeichnis rekursiv nach CSV- und ZIP-Dateien\n",
    "\n",
    "        extracted_files_count = 0\n",
    "        \n",
    "        for root, dirs, files in os.walk(temp_extract_dir):\n",
    "            \n",
    "            if '__MACOSX' in dirs:\n",
    "                dirs.remove('__MACOSX')\n",
    "\n",
    "            for file_name in files:\n",
    "                source_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # a) Wenn es eine ZIP-Datei ist (Szenario 3: Neue Jahresarchive)\n",
    "                if file_name.endswith('.zip'):\n",
    "                    # Entpacke die innere ZIP direkt in das finale Zielverzeichnis (output_dir)\n",
    "                    with zipfile.ZipFile(source_path, 'r') as inner_zf:\n",
    "                        inner_zf.extractall(output_dir)\n",
    "                        extracted_files_count += len([m for m in inner_zf.namelist() if m.endswith('.csv')])\n",
    "                        \n",
    "                # b) Wenn es eine CSV-Datei ist (Szenario 1 oder 2: Alte Jahresarchive oder direkte Monats-ZIPs)\n",
    "                elif file_name.endswith('.csv'):\n",
    "                    # Verschiebe die CSV in das finale Zielverzeichnis (output_dir)\n",
    "                    # Wir überschreiben Duplikate (z.B. wenn es doppelte Benennung in der Hierarchie gab)\n",
    "                    shutil.move(source_path, os.path.join(output_dir, file_name))\n",
    "                    extracted_files_count += 1\n",
    "        \n",
    "        # 4. Cleanup: Lösche das temporäre Verzeichnis\n",
    "        shutil.rmtree(temp_extract_dir)\n",
    "        \n",
    "        return f\"Erfolg: {zip_filename} entpackt. {extracted_files_count} CSVs verschoben/extrahiert.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 5. Cleanup bei Fehler: Versuche, das temporäre Verzeichnis zu löschen\n",
    "        if os.path.exists(temp_extract_dir):\n",
    "             shutil.rmtree(temp_extract_dir)\n",
    "             \n",
    "        return f\"FEHLER: {zip_filename} konnte nicht verarbeitet werden. {e}\"\n",
    "\n",
    "def process_and_save_csv(csv_path):\n",
    "    \"\"\"Liest eine CSV, bereinigt Spaltennamen und speichert als temporäres Parquet.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Spaltennamen-Mapping erstellen (um alte und neue Formate zu vereinheitlichen)\n",
    "        df_temp = pd.read_csv(csv_path, nrows=0) # Nur Header lesen\n",
    "        df_cols = df_temp.columns.tolist()\n",
    "        \n",
    "        # Erzeuge ein Mapping, um alte Spaltennamen auf neue zu vereinheitlichen\n",
    "        column_map = {}\n",
    "        use_cols_final = []\n",
    "\n",
    "        # Nutzerspalte usertype/member_casual ist in verschiedenen Dateien unterschiedlich kodiert\n",
    "        requires_usertype_conversion = False # Flag für die Wertekonvertierung\n",
    "        \n",
    "        for col in df_cols:\n",
    "            # --- Datum/Zeit-Daten standardisieren ---\n",
    "            if 'start_time' in col or 'started_at' in col or 'starttime' in col or 'Start Time' in col:\n",
    "                column_map[col] = 'started_at'\n",
    "                use_cols_final.append('started_at')\n",
    "            elif 'stop_time' in col or 'ended_at' in col or 'stoptime' in col or 'Stop Time' in col:\n",
    "                column_map[col] = 'ended_at'\n",
    "                use_cols_final.append('ended_at')\n",
    "            \n",
    "            # --- Geo-Daten standardisieren ---\n",
    "            elif 'start station latitude' in col or 'start_lat' in col or 'Start Station Latitude' in col:\n",
    "                column_map[col] = 'start_lat'\n",
    "                use_cols_final.append('start_lat')\n",
    "            elif 'end station latitude' in col or 'end_lat' in col or 'End Station Latitude' in col:\n",
    "                column_map[col] = 'end_lat'\n",
    "                use_cols_final.append('end_lat')\n",
    "            elif 'start station longitude' in col or 'start_lng' in col or 'Start Station Longitude' in col:\n",
    "                column_map[col] = 'start_lng'\n",
    "                use_cols_final.append('start_lng')    \n",
    "            elif 'end station longitude' in col or 'end_lng' in col or 'End Station Longitude' in col:\n",
    "                column_map[col] = 'end_lng'\n",
    "                use_cols_final.append('end_lng')    \n",
    "            \n",
    "            # --- Stations Namen/ID standardisieren ---\n",
    "            elif 'start station name' in col or 'start_station_name' in col or 'Start Station Name' in col:\n",
    "                column_map[col] = 'start_station_name'\n",
    "                use_cols_final.append('start_station_name')    \n",
    "            elif 'end station name' in col or 'end_station_name' in col or 'End Station Name' in col:\n",
    "                column_map[col] = 'end_station_name'\n",
    "                use_cols_final.append('end_station_name')      \n",
    "            elif 'start station id' in col or 'start_station_id' in col or 'Start Station ID' in col:\n",
    "                column_map[col] = 'start_station_id'\n",
    "                use_cols_final.append('start_station_id')    \n",
    "            elif 'end station id' in col or 'end_station_id' in col or 'End Station ID' in col:\n",
    "                column_map[col] = 'end_station_id'\n",
    "                use_cols_final.append('end_station_id')    \n",
    "            \n",
    "            # --- Rideable/Bike Typ standardisieren ---\n",
    "            elif 'rideable_type' in col:\n",
    "                column_map[col] = 'rideable_type'\n",
    "                use_cols_final.append('rideable_type') \n",
    "            \n",
    "\n",
    "            # --- Nutzerspalte standardisieren ---\n",
    "            elif 'member_casual' in col:\n",
    "                column_map[col] = 'member_casual'\n",
    "                use_cols_final.append('member_casual')\n",
    "            elif 'usertype' in col or 'User Type' in col:\n",
    "                column_map[col] = 'member_casual'\n",
    "                use_cols_final.append('member_casual')\n",
    "                requires_usertype_conversion = True\n",
    "            else:\n",
    "                print(f\"Unbekannte Spalte gefunden: {col}. Bitte erweitern Sie das Mapping.\")\n",
    "        \n",
    "        \n",
    "      \n",
    "        # Entferne Duplikate aus use_cols_final\n",
    "        use_cols_final = list(set(use_cols_final))\n",
    "        \n",
    "            \n",
    "\n",
    "        # 2. Datei einlesen\n",
    "        df = pd.read_csv(csv_path, \n",
    "                            usecols=list(column_map.keys()), \n",
    "                            dtype=DTYPE_MAP,\n",
    "                            low_memory=False)\n",
    "\n",
    "        # 2019 wurden bei Citibike die E-Bikes eingeführt. Davor gab es nur klassische Bikes. Daher fügen wir diese Spalte bei alten Daten hinzu.\n",
    "        if 'rideable_type' not in df.columns:\n",
    "            df['rideable_type'] = 'classic_bike'\n",
    "\n",
    "        # Problematisch - Konvertiere Station-IDs zu integer (ungültige Strings wie 'SYS016' werden zu NaN)\n",
    "        # for id_col in ['start_station_id', 'end_station_id']:\n",
    "        #     if id_col in ddf.columns:\n",
    "        #         df[id_col] = pd.to_numeric(df[id_col], errors='coerce').astype('Int16')\n",
    "\n",
    "        # 3. Spalten umbenennen \n",
    "        df.rename(columns=column_map, inplace=True)\n",
    "        \n",
    "        # Anpassung der Nutzerspalte falls erforderlich\n",
    "        if requires_usertype_conversion:\n",
    "                    df['member_casual'] = df['member_casual'].astype('object').replace({'Subscriber': 'member', 'Customer': 'casual'})\n",
    "\n",
    "        df['member_casual'] = df['member_casual'].astype('category')\n",
    "        # 4. Nur die vereinheitlichten Spalten beibehalten\n",
    "        #   Einträge die keinen Mehrwert bieten oder die nur in alten Daten existieren  wurden nicht zu use_cols_final hinzugefügt\n",
    "        #   2014-2019: tripduration,bikeid,gender,birth year\n",
    "        #   2020-2025: ride_id\n",
    "        \n",
    "\n",
    "        # Diese Spalten liegen teils als rein numerische und teils als alpha-numerische Daten vor und müssen absolut konsistent als String vorliegen\n",
    "        id_cols_to_fix = ['start_station_id', 'end_station_id']\n",
    "        for col in id_cols_to_fix:\n",
    "            if col in df.columns:\n",
    "                # 1. Alles in Strings umwandeln (auch NaNs werden zu 'nan')\n",
    "                df[col] = df[col].astype(str)\n",
    "                                \n",
    "                # 2. 'nan' Strings (von echten NaNs) wieder zu echten Python-None machen\n",
    "                #    Das erlaubt PyArrow, sie als NULL-Werte im String-Schema zu speichern\n",
    "                df[col] = df[col].replace(['nan', 'NaN', 'None', ''], None)\n",
    "        \n",
    "        \n",
    "        df = df[use_cols_final]\n",
    "\n",
    "        # 5. Speichern als temporäres Parquet\n",
    "        temp_parquet_path = os.path.join(PROCESSED_DATA_DIR, f'temp_{os.path.basename(csv_path)}.parquet')\n",
    "        df.to_parquet(temp_parquet_path, index=False)\n",
    "\n",
    "        \n",
    "        # 6. Speicher freigeben\n",
    "        del df\n",
    "        return f\"Erfolg: {os.path.basename(csv_path)} verarbeitet und als Parquet gespeichert.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"FEHLER beim Verarbeiten: {os.path.basename(csv_path)}. {e}\"\n",
    "    \n",
    "def orchestrate_data_pipeline():\n",
    "    \n",
    "    # 1. Ordner erstellen\n",
    "    os.makedirs(UNZIPPED_DIR, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # 2. Alle heruntergeladenen ZIP-Dateien finden\n",
    "    all_zip_files = glob.glob(os.path.join(RAW_DATA_DIR, '*.zip'))\n",
    "\n",
    "    if not all_zip_files:\n",
    "        print(f\"KRITISCHER FEHLER: Keine ZIP-Dateien im Ordner {RAW_DATA_DIR} gefunden.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Starte Verarbeitung von {len(all_zip_files)} ZIP-Dateien...\")\n",
    "        \n",
    "    # 3. Alle ZIPs parallel entpacken\n",
    "    print(\"\\n--- Phase 1: Entpacken der ZIPs ---\")\n",
    "    # Wir filtern die 'Überspringen'-Meldungen in der tqdm-Schleife für eine saubere Ausgabe.\n",
    "    skipped_count = 0\n",
    "    # ThreadPoolExecutor anstatt ProcessPoolExecutor (macht Probleme) verwenden\n",
    "    # with ThreadPoolExecutor(max_workers=32) as executor: \n",
    "    #     futures = [executor.submit(unzip_file, zp, UNZIPPED_DIR) for zp in all_zip_files]\n",
    "    #     for future in tqdm(as_completed(futures), total=len(all_zip_files), desc=\"Entpacke\"):\n",
    "    #         result = future.result()\n",
    "    #         if \"FEHLER\" in result:\n",
    "    #             print(f\"\\n{result}\")\n",
    "    #         elif \"Überspringe\" in result:\n",
    "    #             skipped_count += 1\n",
    "\n",
    "    # TEST: Sequenzielle Schleife nutzen, falls Multithreading/-processing Probleme macht:\n",
    "    # print(\"\\n--- Phase 1: Sequenzieller Testlauf Entpacken ---\")\n",
    "    # skipped_count = 0\n",
    "    # for zip_path in tqdm(all_zip_files, desc=\"Entpacke Sequenziell\"):\n",
    "    #     result = unzip_file(zip_path, UNZIPPED_DIR)\n",
    "    #     if \"FEHLER\" in result:\n",
    "    #         print(f\"\\n{result}\")\n",
    "    #         # Beenden Sie hier, um den Fehler zu sehen\n",
    "    #         raise Exception(\"Sequenzieller Entpack-Fehler aufgetreten.\")\n",
    "    #     elif \"Überspringe\" in result:\n",
    "    #         skipped_count += 1\n",
    "\n",
    "    if skipped_count > 0:\n",
    "        print(f\"INFO: {skipped_count} ZIP-Dateien wurden übersprungen, da die entpackten Daten bereits existieren.\")\n",
    "\n",
    "    # 4. Alle entpackten CSVs finden\n",
    "    all_csv_files = glob.glob(os.path.join(UNZIPPED_DIR, '*.csv'))\n",
    "    print(f\"\\n{len(all_csv_files)} CSV-Dateien gefunden und bereit zur Parallelverarbeitung.\")\n",
    "    \n",
    "    # 5. CSVs verarbeiten und als temporäre Parquet-Files speichern\n",
    "\n",
    "    # Parallel verarbeiten macht Probleme \n",
    "    # print(\"\\n--- Phase 2: Parallelverarbeitung der CSVs ---\")\n",
    "    # with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    #     futures = [executor.submit(process_and_save_csv, csvp) for csvp in all_csv_files]\n",
    "    #     for future in tqdm(as_completed(futures), total=len(all_csv_files), desc=\"Verarbeite CSVs\"):\n",
    "    #         result = future.result()\n",
    "    #         if \"FEHLER\" in result:\n",
    "    #             print(f\"\\n{result}\")\n",
    "    \n",
    "\n",
    "    # Sequenzielle Schleife nutzen, da Multiprocessing Probleme macht (auskommentiert um diesen Prozess nur einmal zu durchlaufen):\n",
    "    print(\"\\n--- Phase 2: Sequenzielle Verarbeitung der CSVs ---\")\n",
    "    # for csv_path in tqdm(all_csv_files, desc=\"Verarbeite CSVs Sequenziell\"):\n",
    "    #     result = process_and_save_csv(csv_path)\n",
    "    #     if \"FEHLER\" in result:\n",
    "    #         print(f\"\\n{result}\")\n",
    "    #         # Wenn hier ein Fehler auftritt, ist es ein Daten-/Code-Fehler, kein ProcessPool-Problem!\n",
    "    #         raise Exception(\"Sequenzieller Verarbeitungsfehler aufgetreten.\")            \n",
    "    \n",
    "    # 6. Konsolidierung & Speichern\n",
    "    print(\"\\n--- Phase 3: Finale Konsolidierung ---\")\n",
    "    final_parquet_files = glob.glob(os.path.join(PROCESSED_DATA_DIR, 'temp_*.parquet'))\n",
    "    print(f\"Füge {len(final_parquet_files)} temporäre Parquet-Dateien zusammen...\")\n",
    "    \n",
    "    # Verwende Pandas.Concat für speichereffiziente Konsolidierung\n",
    "    # df_final = pd.concat([pd.read_parquet(f) for f in tqdm(final_parquet_files, desc=\"Lade Parquet Chunks\")], \n",
    "    #                     ignore_index=True)\n",
    "    # df_final.to_parquet(FINAL_PARQUET_PATH, index=False)\n",
    "\n",
    "    # Verwende PyArrow für speichereffiziente Konsolidierung, um Memory Overflow zu vermeiden\n",
    "    # --> Immernoch zu hohe Speicherauslastung --> Dask Parquet \n",
    "    # for f in tqdm(final_parquet_files, desc=\"Lade Parquet Chunks\"):\n",
    "    #     table = pq.read_table(f)\n",
    "    #     tables.append(table)\n",
    "    # # Kombiniere alle geladenen Tabellen\n",
    "    # combined_table = pa.concat_tables(tables)\n",
    "    # # Schreibe die kombinierte Tabelle in die Zieldatei\n",
    "    # pq.write_table(combined_table, FINAL_PARQUET_PATH)\n",
    "\n",
    "\n",
    "    \n",
    "    # Verwende Dask Parquet für speichereffiziente Konsolidierung, um Memory Overflow zu vermeiden\n",
    "    print(f\"Lese {len(final_parquet_files)} Parquet-Dateien (lazy)...\")\n",
    "    ddf = dd.read_parquet(final_parquet_files)\n",
    "    print(f\"Schreibe die kombinierte Tabelle nach {FINAL_PARQUET_PATH}...\")\n",
    "    ddf.to_parquet(FINAL_PARQUET_PATH, write_index=False)\n",
    " \n",
    "    print(f\"\\n--- ERFOLG ---\")\n",
    "    print(f\"Gesamtdatensatz gespeichert unter: {FINAL_PARQUET_PATH}\")\n",
    "           \n",
    "    # 7. Aufräumen (optional)\n",
    "    # Entfernen Sie die temporären Dateien, um Speicherplatz zu sparen\n",
    "    # for f in final_parquet_files: os.remove(f)\n",
    "    # for f in all_csv_files: os.remove(f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Achtung: Die Ausführung dieses Skripts kann bei allen Jahren sehr lange dauern!\n",
    "    orchestrate_data_pipeline()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ddb89",
   "metadata": {},
   "source": [
    "NYPD CRASH DATA verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37877630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lese NYPD-Daten ein von: ../data/unzipped/nypd/Motor_Vehicle_Collisions_-_Crashes_20251209.csv\n",
      "\n",
      "=======================================================\n",
      "\n",
      "NYPD-Daten erfolgreich geladen. 2,226,246 Zeilen.\n",
      "=======================================================\n",
      "   CRASH DATE CRASH TIME   BOROUGH ZIP CODE  LATITUDE  LONGITUDE  \\\n",
      "0  09/11/2021       2:39       NaN      NaN       NaN        NaN   \n",
      "1  03/26/2022      11:45       NaN      NaN       NaN        NaN   \n",
      "2  11/01/2023       1:29  BROOKLYN    11230  40.62179 -73.970024   \n",
      "3  06/29/2022       6:55       NaN      NaN       NaN        NaN   \n",
      "4  09/21/2022      13:21       NaN      NaN       NaN        NaN   \n",
      "\n",
      "                     LOCATION           ON STREET NAME CROSS STREET NAME  \\\n",
      "0                         NaN    WHITESTONE EXPRESSWAY         20 AVENUE   \n",
      "1                         NaN  QUEENSBORO BRIDGE UPPER               NaN   \n",
      "2      (40.62179, -73.970024)            OCEAN PARKWAY          AVENUE K   \n",
      "3                         NaN       THROGS NECK BRIDGE               NaN   \n",
      "4                         NaN          BROOKLYN BRIDGE               NaN   \n",
      "\n",
      "  OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
      "0             NaN  ...                    Unspecified   \n",
      "1             NaN  ...                            NaN   \n",
      "2             NaN  ...                    Unspecified   \n",
      "3             NaN  ...                    Unspecified   \n",
      "4             NaN  ...                    Unspecified   \n",
      "\n",
      "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
      "0                            NaN                            NaN   \n",
      "1                            NaN                            NaN   \n",
      "2                    Unspecified                            NaN   \n",
      "3                            NaN                            NaN   \n",
      "4                            NaN                            NaN   \n",
      "\n",
      "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  \\\n",
      "0                            NaN       4455765   \n",
      "1                            NaN       4513547   \n",
      "2                            NaN       4675373   \n",
      "3                            NaN       4541903   \n",
      "4                            NaN       4566131   \n",
      "\n",
      "                   VEHICLE TYPE CODE 1  VEHICLE TYPE CODE 2  \\\n",
      "0                                Sedan                Sedan   \n",
      "1                                Sedan                  NaN   \n",
      "2                                Moped                Sedan   \n",
      "3                                Sedan        Pick-up Truck   \n",
      "4  Station Wagon/Sport Utility Vehicle                  NaN   \n",
      "\n",
      "   VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5  \n",
      "0                  NaN                 NaN                 NaN  \n",
      "1                  NaN                 NaN                 NaN  \n",
      "2                Sedan                 NaN                 NaN  \n",
      "3                  NaN                 NaN                 NaN  \n",
      "4                  NaN                 NaN                 NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "## Einlesen Datenbank 1: NYPD Motor Vehicle Collisions\n",
    "\n",
    "NYPD_IN_FILE = 'Motor_Vehicle_Collisions_-_Crashes_20251209.csv'\n",
    "nypd_path = os.path.join('../data/unzipped/nypd/', NYPD_IN_FILE)\n",
    "\n",
    "print(f\"Lese NYPD-Daten ein von: {nypd_path}\")\n",
    "\n",
    "try:\n",
    "    # Nur die relevantesten Spalten zur Optimierung der Ladezeit auswählen\n",
    "    nypd_cols = [\"CRASH DATE\",\"CRASH TIME\",\"BOROUGH\",\"ZIP CODE\",\"LATITUDE\",\"LONGITUDE\",\"LOCATION\",\"ON STREET NAME\",\"CROSS STREET NAME\",\"OFF STREET NAME\",\"NUMBER OF PERSONS INJURED\",\"NUMBER OF PERSONS KILLED\",\"NUMBER OF PEDESTRIANS INJURED\",\"NUMBER OF PEDESTRIANS KILLED\",\"NUMBER OF CYCLIST INJURED\",\"NUMBER OF CYCLIST KILLED\",\"NUMBER OF MOTORIST INJURED\",\"NUMBER OF MOTORIST KILLED\",\"CONTRIBUTING FACTOR VEHICLE 1\",\"CONTRIBUTING FACTOR VEHICLE 2\",\"CONTRIBUTING FACTOR VEHICLE 3\",\"CONTRIBUTING FACTOR VEHICLE 4\",\"CONTRIBUTING FACTOR VEHICLE 5\",\"COLLISION_ID\",\"VEHICLE TYPE CODE 1\",\"VEHICLE TYPE CODE 2\",\"VEHICLE TYPE CODE 3\",\"VEHICLE TYPE CODE 4\",\"VEHICLE TYPE CODE 5\"]\n",
    "    #   nypd_cols = ['CRASH DATE', 'CRASH TIME', 'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF CYCLIST INJURED','VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']\n",
    "\n",
    "    df_nypd = pd.read_csv(nypd_path, usecols=lambda x: x in nypd_cols, encoding='latin1', low_memory=False)\n",
    "#     ddf_nypd = dd.read_csv(\n",
    "#     nypd_path, \n",
    "#     usecols=nypd_cols,    # Dask akzeptiert die Liste nypd_cols direkt\n",
    "#     encoding='latin1', \n",
    "#     # low_memory=False,   # In Dask nicht nötig/vorhanden, da Dask sowieso chunkweise arbeitet\n",
    "#     dtype='object'         # Empfehlung: Erstmal als object laden, um Typ-Konflikte zu vermeiden\n",
    "# )\n",
    "    print(f\"\\n=======================================================\")\n",
    "    print(f\"\\nNYPD-Daten erfolgreich geladen. {len(df_nypd):,} Zeilen.\")\n",
    "    print(f\"=======================================================\")\n",
    "    print(df_nypd.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"FEHLER: Datei {NYPD_IN_FILE} nicht im angegebenen Pfad gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79665edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYPD-Daten (unbereinigt): 1,985,841 Zeilen.\n",
      "NYPD-Daten nach Geo-Bereinigung: 1,985,841 Zeilen.\n",
      "NYPD-Daten nach Datetime-Konvertierung und -Bereinigung: 1,985,841 Zeilen.\n",
      "\n",
      "NYPD-Daten nach Filterung auf Fahrrad-Unfälle: 84,361 Zeilen.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\data\\processed\\nypd'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     62\u001b[39m df_nypd_clean = df_nypd_filtered[[\n\u001b[32m     63\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcrash_datetime\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     64\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLATITUDE\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mVEHICLE TYPE CODE 5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     73\u001b[39m ]].copy()\n\u001b[32m     75\u001b[39m nypd_parquet_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33m../data/processed/nypd\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnypd_clean.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mdf_nypd_clean\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnypd_parquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=======================================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNYPD-Daten erfolgreich bereinigt. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_nypd_clean)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Zeilen.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m impl = get_engine(engine)\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io.BytesIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\io\\parquet.py:199\u001b[39m, in \u001b[36mPyArrowImpl.write\u001b[39m\u001b[34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m     merged_metadata = {**existing_metadata, **df_metadata}\n\u001b[32m    197\u001b[39m     table = table.replace_schema_metadata(merged_metadata)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    207\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io.BufferedWriter)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle.name, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tstue\\miniforge3\\envs\\axa-citibike-challenge\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '..\\data\\processed\\nypd'"
     ]
    }
   ],
   "source": [
    "## Bereinigen Datenbank 1: NYPD Motor Vehicle Collisions\n",
    "os.makedirs('../data/processed/nypd', exist_ok=True)\n",
    "print(f\"NYPD-Daten (unbereinigt): {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "# Koordinaten filtern (entfernt Zeilen ohne Geo-Daten)\n",
    "df_nypd.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "print(f\"NYPD-Daten nach Geo-Bereinigung: {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "# Crash Date/Time in Datetime-Typ konvertieren; Die Fehler-Toleranz (errors='coerce') setzt ungültige Daten auf NaT (Not a Time)\n",
    "df_nypd['CRASH DATE'] = pd.to_datetime(df_nypd['CRASH DATE'], errors='coerce')\n",
    "\n",
    "# Prüfen, ob die 'CRASH TIME'-Spalte bereits datetime.time-Objekte enthält.\n",
    "# Dies verhindert, dass pd.to_datetime bei bereits konvertierten Objekten fehlschlägt.\n",
    "if not df_nypd['CRASH TIME'].dropna().empty and \\\n",
    "   isinstance(df_nypd['CRASH TIME'].dropna().iloc[0], dt.time):\n",
    "    pass\n",
    "else:\n",
    "    df_nypd['CRASH TIME'] = pd.to_datetime(df_nypd['CRASH TIME'], format='%H:%M', errors='coerce').dt.time\n",
    "\n",
    "\n",
    "# Die CRASH TIME liegt als Python 'time' Objekt vor, das nicht direkt addiert werden kann.\n",
    "def time_to_seconds(t):\n",
    "    \"\"\"Konvertiert ein datetime.time Objekt in die Gesamtanzahl der Sekunden.\"\"\"\n",
    "    # fängt fehlerhafte time Objekte ab\n",
    "    if pd.isna(t):\n",
    "        return pd.NA\n",
    "    return t.hour * 3600 + t.minute * 60 + t.second\n",
    "\n",
    "# Konvertiere in Sekunden, bilde ein Zeitdelta und addiere dieses zum Datum\n",
    "df_nypd['time_seconds'] = df_nypd['CRASH TIME'].apply(time_to_seconds)\n",
    "df_nypd['time_delta'] = pd.to_timedelta(df_nypd['time_seconds'], unit='s')\n",
    "df_nypd['crash_datetime'] = df_nypd['CRASH DATE'] + df_nypd['time_delta']\n",
    "\n",
    "# Entferne die temporären Spalten und Zeilen, die nach der Kombination ungültig sind.\n",
    "df_nypd.drop(columns=['time_seconds', 'time_delta'], inplace=True)\n",
    "df_nypd.dropna(subset=['crash_datetime'], inplace=True)\n",
    "print(f\"NYPD-Daten nach Datetime-Konvertierung und -Bereinigung: {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "\n",
    "# Bereinigung: Unfallbeteiligte filtern (entfernt Zeilen ohne  Fahrradbeteiligung)\n",
    "# -- Fall A: Prüfen, ob \"bike\" / \"bicycle\" in den Fahrzeugtyp-Codes vorkommt\n",
    "BIKE_IDENTIFIER = \"bike|bicycle\"\n",
    "\n",
    "bicycle_in_codes = (\n",
    "    df_nypd['VEHICLE TYPE CODE 1'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 2'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 3'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 4'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 5'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False)\n",
    ")\n",
    "\n",
    "# -- Fall B: Prüfen, ob eine Person als Radfahrer verletzt oder getötet wurde\n",
    "# (Stellt sicher, dass auch nicht explizit als \"bike\" / \"bicycle\"  gekennzeichnete, aber involvierte Fahrräder erfasst werden)\n",
    "cyclist_injured = df_nypd['NUMBER OF CYCLIST INJURED'].fillna(0).astype(int) > 0\n",
    "cyclist_killed = df_nypd['NUMBER OF CYCLIST KILLED'].fillna(0).astype(int) > 0\n",
    "\n",
    "# Kombiniere die beiden Fälle (OR-Verknüpfung)\n",
    "df_nypd_filtered = df_nypd[bicycle_in_codes | cyclist_injured | cyclist_killed].copy()\n",
    "print(f\"\\nNYPD-Daten nach Filterung auf Fahrrad-Unfälle: {len(df_nypd_filtered):,} Zeilen.\")\n",
    "\n",
    "# Reduziere den DataFrame auf die wesentlichen Geo- und Zeit-Spalten für die Analyse\n",
    "df_nypd_clean = df_nypd_filtered[[\n",
    "    'crash_datetime',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'NUMBER OF CYCLIST INJURED',\n",
    "    'NUMBER OF CYCLIST KILLED',\n",
    "    'VEHICLE TYPE CODE 1',\n",
    "    'VEHICLE TYPE CODE 2',\n",
    "    'VEHICLE TYPE CODE 3',\n",
    "    'VEHICLE TYPE CODE 4', \n",
    "    'VEHICLE TYPE CODE 5'\n",
    "]].copy()\n",
    "\n",
    "nypd_parquet_path = os.path.join('../data/processed/nypd', 'nypd_clean.parquet')\n",
    "df_nypd_clean.to_parquet(nypd_parquet_path, index=False)\n",
    "\n",
    "print(f\"\\n=======================================================\")\n",
    "print(f\"NYPD-Daten erfolgreich bereinigt. {len(df_nypd_clean):,} Zeilen.\")\n",
    "print(f\"=======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259aa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axa-citibike-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
