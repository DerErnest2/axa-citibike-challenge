{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3420079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Pandas für Datenmanipulation\n",
    "import dask.dataframe as dd # Dask für große Datenmengen\n",
    "import zipfile\n",
    "import requests # HTTP-Anfragen\n",
    "import shutil # Löschen von Ordnern\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed # Multithreading-Unterstützung\n",
    "import os\n",
    "from tqdm import tqdm # Fortschrittsbalken\n",
    "import time\n",
    "import glob # Dateimustererkennung\n",
    "import re\n",
    "import xml.etree.ElementTree as ET # Der native XML-Parser\n",
    "import pyarrow as pa # \n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e61ef",
   "metadata": {},
   "source": [
    "CITIBIKE DATEN - HERUNTERLADEN & VERARBEITEN\n",
    "1. ZIP Download Muster generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0726d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Jahre/Muster, die Sie herunterladen möchten. Mögliche Formate:\n",
    "# 1. \"*\"\" - Alle Jahre \n",
    "# 2. \"YYYY-YYYY\" - Jahr-Bereich (z.B. [2015-2017])\n",
    "# 3. [2016, 2017, 2018] - Spezifische Jahre\n",
    "TARGET_YEARS = [2014]\n",
    "\n",
    "# --- Generierung der Download-Muster basierend auf TARGET_YEARS ---\n",
    "def generate_download_patterns(target_input):\n",
    " \n",
    "    years = []\n",
    "    current_year = datetime.now().year\n",
    "    first_data_year = 2013 # Startjahr der CitiBike-Daten\n",
    "\n",
    "    # --- Verarbeitung der Eingabe ---\n",
    "    if isinstance(target_input, list):\n",
    "        # Format: [2016, 2017]\n",
    "        years = target_input\n",
    "    \n",
    "    elif isinstance(target_input, str) and target_input == \"*\":\n",
    "        # Format: \"*\"\" -> Alle Jahre\n",
    "        years = list(range(first_data_year, current_year + 1))\n",
    "        \n",
    "    elif isinstance(target_input, str) and re.match(r\"^\\d{4}-\\d{4}$\", target_input):\n",
    "        # Format: \"YYYY-YYYY\" -> Jahr-Bereich\n",
    "        try:\n",
    "            start_year, end_year = map(int, target_input.split('-'))\n",
    "            if start_year > end_year:\n",
    "                raise ValueError(\"Startjahr muss kleiner oder gleich dem Endjahr sein.\")\n",
    "            years = list(range(start_year, end_year + 1))\n",
    "        except ValueError as e:\n",
    "            print(f\"FEHLER beim Parsen des Jahresbereichs: {e}\")\n",
    "            return []\n",
    "            \n",
    "    else:\n",
    "        print(\"FEHLER: Ungültiges TARGET_YEARS Format. Erlaubt sind: [*], [YYYY-YYYY] oder eine Liste von Jahren [2016, 2017].\")\n",
    "        return []\n",
    "\n",
    "    # Ungültige/Zukünftige Jahre filtern\n",
    "    years = [y for y in years if first_data_year <= y <= current_year]\n",
    "    \n",
    "    if not years:\n",
    "        print(\"Keine gültigen Jahre gefunden, die heruntergeladen werden können.\")\n",
    "        return []\n",
    "\n",
    "    # --- Generierung der Regex-Muster ---\n",
    "    patterns = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_str = str(year)\n",
    "        # Muster für monatliche Dateien (202301-...) und ältere/JC-Formate\n",
    "        patterns.append(rf\"^{year_str}\\d{{2}}-.*\\.zip$\")\n",
    "        patterns.append(rf\"^{year_str}-.*\\.zip$\")\n",
    "        patterns.append(rf\"^JC-{year_str}\\d{{2}}-.*\\.zip$\")\n",
    "        \n",
    "    # Entferne Duplikate\n",
    "    return list(set(patterns))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    YEAR_PATTERNS = generate_download_patterns(TARGET_YEARS)\n",
    "    print(\"Generierte Download-Muster:\", YEAR_PATTERNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee774c9f",
   "metadata": {},
   "source": [
    "2. ZIP Dateien herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://s3.amazonaws.com/tripdata/\"  # S3-Indexseite mit den Citibike-Daten\n",
    "RAW_DATA_DIR = \"../data/raw/citibike\" \n",
    "\n",
    "# --- Setup ---\n",
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "    os.makedirs(RAW_DATA_DIR)\n",
    "    print(f\"Lokaler Download-Ordner erstellt: {RAW_DATA_DIR}\")\n",
    "\n",
    "\n",
    "def find_and_download_files():\n",
    "    \"\"\"Crawlt die S3-Seite und lädt alle relevanten ZIP-Dateien herunter.\"\"\"\n",
    "    print(f\"Starte Crawling der S3-Seite: {BASE_URL}\")\n",
    "      \n",
    "    try:\n",
    "        # Führe eine GET-Anfrage an die S3-Indexseite durch\n",
    "        response = requests.get(BASE_URL)\n",
    "        print(f\"HTTP-Statuscode der Antwort: {response.status_code}\")\n",
    "        response.raise_for_status() # Löst Fehler bei ungültigem Status (4xx, 5xx) aus\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"FEHLER beim Zugriff auf die S3-URL: {e}\")\n",
    "        return\n",
    "\n",
    "    download_list = []\n",
    "\n",
    "    # # Parse den XML-Inhalt hat weder mit html.parser noch mit lxml-xml funktioniert\n",
    "    # # soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "    \n",
    "    # Alternative Methode: Verwende den nativen XML-Parser\n",
    "    try:\n",
    "        # 1. Parse den XML-Inhalt mit ElementTree\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # 2. S3 verwendet Namespaces; wir müssen den Namespace aus dem Root-Tag extrahieren\n",
    "        # Beispiel: {http://s3.amazonaws.com/doc/2006-03-01/}\n",
    "        namespace = re.match(r'\\{.*\\}', root.tag).group(0)\n",
    "        \n",
    "        # 3. Finde alle 'Key'-Tags innerhalb des S3-Listings\n",
    "        # Der Tag-Name muss mit dem extrahierten Namespace verwendet werden\n",
    "        keys = root.findall(f'.//{namespace}Key')\n",
    "\n",
    "        print(f\"Gefundene <Key>-Elemente im XML: {len(keys)}\")\n",
    "        \n",
    "        # 4. Filterung und Sammeln der Dateinamen\n",
    "        for key in keys:\n",
    "            filename = key.text\n",
    "            \n",
    "            if not filename or not filename.endswith(\".zip\"):\n",
    "                continue\n",
    "                \n",
    "            is_relevant = False\n",
    "            for pattern in YEAR_PATTERNS:\n",
    "                if any(re.match(pattern.replace('*', '.*'), filename) for pattern in YEAR_PATTERNS):\n",
    "                # if re.match(pattern, filename): \n",
    "                    is_relevant = True\n",
    "                    break\n",
    "            \n",
    "            if is_relevant:\n",
    "                print(f\"Gefundene relevante Datei: {filename}\")\n",
    "                download_list.append(filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"KRITISCHER FEHLER beim Parsen der S3-Antwort: {e}\")\n",
    "        return\n",
    "    if not download_list:\n",
    "        print(\"Keine Dateien gefunden, die den Suchmustern entsprechen. Prüfen Sie die YEAR_PATTERNS.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Insgesamt {len(download_list)} Dateien zum Herunterladen gefunden.\")\n",
    "    \n",
    "    # Starte den Download-Prozess\n",
    "    for filename in download_list:\n",
    "        file_url = BASE_URL + filename\n",
    "        local_path = os.path.join(RAW_DATA_DIR, filename)\n",
    "\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"  > Überspringe: {filename} (existiert bereits)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  > Downloade: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Streaming-Download, um den Speicher nicht zu überlasten\n",
    "            with requests.get(file_url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                \n",
    "                with open(local_path, 'wb') as f:\n",
    "                    # Fortschrittsbalken mit tqdm\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename) as t:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                            t.update(len(chunk))\n",
    "            \n",
    "            # Kurze Pause, um den Server nicht zu überlasten\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  FEHLER beim Download von {filename}: {e}\")\n",
    "            time.sleep(5) # Längere Pause bei Fehlern\n",
    "            continue\n",
    "\n",
    "    print(\"\\n--- Download abgeschlossen ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82040905",
   "metadata": {},
   "source": [
    "3. CSV Dateien bereinigen und in Dask-Format umwandeln\n",
    "\n",
    "    Spaltennamen sind nicht konsistent: <br>\n",
    "- 2014-2019: tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
    "\n",
    "- 2020-2025: ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa04db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNZIPPED_DIR = \"../data/unzipped/citibike\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed/citibike\"\n",
    "FINAL_PARQUET_PATH = os.path.join(PROCESSED_DATA_DIR, 'citibike_all_years_combined.parquet')\n",
    "\n",
    "# Daten-Typen-Optimierung um Speicher (RAM) zu sparen\n",
    "# WICHTIG: Datums- und Zeit-Spalten werden NICHT in der DTYPE_MAP auf datetime\n",
    "# gesetzt, da dies in read_csv zu Problemen führt.\n",
    "\n",
    "DTYPE_MAP = {\n",
    "    # --- Geo-Koordinaten (Alt & Neu) ---\n",
    "    'start_lat': 'float32',\n",
    "    'start_lng': 'float32',\n",
    "    'end_lat': 'float32',\n",
    "    'end_lng': 'float32',\n",
    "    'start station latitude': 'float32',\n",
    "    'start station longitude': 'float32',\n",
    "    'end station latitude': 'float32',\n",
    "    'end station longitude': 'float32',\n",
    "    'Start Station Latitude': 'float32',\n",
    "    'Start Station Longitude': 'float32',\n",
    "    'End Station Latitude': 'float32',\n",
    "    'End Station Longitude': 'float32',\n",
    "    \n",
    "    # --- IDs ---\n",
    "    'ride_id': 'string',\n",
    "    'bikeid': 'int32',         # Alte bikeid ist oft nur eine Zahl\n",
    "    'Bike ID': 'int32',\n",
    "    'start_station_id': 'string',\n",
    "    'end_station_id': 'string',\n",
    "    'start station id': 'string', \n",
    "    'end station id': 'string',\n",
    "    'Start Station Id': 'string',\n",
    "    'End Station Id': 'string',\n",
    "    \n",
    "    # --- Typen und Kategorien ---\n",
    "    'rideable_type': 'category',\n",
    "    'usertype': 'category',\n",
    "    'User Type': 'category',\n",
    "    'member_casual': 'category',\n",
    "    'gender': 'category',\n",
    "    'Gender': 'category',\n",
    "    \n",
    "    # --- Numerische Werte ---\n",
    "    'tripduration': 'int32',\n",
    "    'Trip Duration': 'int32',\n",
    "    'birth year': 'float16', # sollte in Int umgewandelt werden\n",
    "    'Birth Year': 'float16', # sollte in Int umgewandelt werden\n",
    "    \n",
    "    # --- Stationsnamen  ---\n",
    "    'start station name': 'string', \n",
    "    'end station name': 'string', \n",
    "    'start_station_name': 'string',\n",
    "    'end_station_name': 'string',\n",
    "    'Start Station Name': 'string',\n",
    "    'End Station Name': 'string',\n",
    "    }\n",
    "\n",
    "\n",
    "# 2014-2018: tripduration,starttime,stoptime,start station id,start station name,start station latitude,start station longitude,end station id,end station name,end station latitude,end station longitude,bikeid,usertype,birth year,gender\n",
    "# 2090-2025: ride_id,rideable_type,started_at,ended_at,start_station_name,start_station_id,end_station_name,end_station_id,start_lat,start_lng,end_lat,end_lng,member_casual\n",
    "\n",
    "def unzip_file(zip_path, output_dir):\n",
    "    \"\"\"\n",
    "    Entpackt eine ZIP-Datei und verarbeitet alle darin enthaltenen Strukturen: \n",
    "    verschachtelte Ordner, verschachtelte ZIPs und direkte CSVs.\n",
    "    \"\"\"\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    \n",
    "    temp_extract_dir = os.path.join(output_dir, f\"temp_extract_{zip_filename.replace('.zip', '')}\")\n",
    "    os.makedirs(temp_extract_dir, exist_ok=True) # Temporäres Verzeichnis für das Entpacken\n",
    "\n",
    "    try:\n",
    "        # Entpacke das Hauptarchiv in ein temporäres Verzeichnis\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(temp_extract_dir)\n",
    "\n",
    "        # Durchsuche das temporäre Verzeichnis rekursiv nach CSV- und ZIP-Dateien\n",
    "        extracted_files_count = 0\n",
    "        \n",
    "        for root, dirs, files in os.walk(temp_extract_dir):\n",
    "            \n",
    "            if '__MACOSX' in dirs:\n",
    "                dirs.remove('__MACOSX')\n",
    "\n",
    "            for file_name in files:\n",
    "                source_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # a) Wenn es eine ZIP-Datei ist (Szenario 3: Neue Jahresarchive)\n",
    "                if file_name.endswith('.zip'):\n",
    "                    # Entpacke die innere ZIP direkt in das finale Zielverzeichnis (output_dir)\n",
    "                    with zipfile.ZipFile(source_path, 'r') as inner_zf:\n",
    "                        inner_zf.extractall(output_dir)\n",
    "                        extracted_files_count += len([m for m in inner_zf.namelist() if m.endswith('.csv')])\n",
    "                        \n",
    "                # b) Wenn es eine CSV-Datei ist (Szenario 1 oder 2: Alte Jahresarchive oder direkte Monats-ZIPs)\n",
    "                elif file_name.endswith('.csv'):\n",
    "                    # Verschiebe die CSV in das finale Zielverzeichnis (output_dir)\n",
    "                    # Wir überschreiben Duplikate (z.B. wenn es doppelte Benennung in der Hierarchie gab)\n",
    "                    shutil.move(source_path, os.path.join(output_dir, file_name))\n",
    "                    extracted_files_count += 1\n",
    "        \n",
    "        # Cleanup: Lösche das temporäre Verzeichnis\n",
    "        shutil.rmtree(temp_extract_dir)\n",
    "        \n",
    "        return f\"Erfolg: {zip_filename} entpackt. {extracted_files_count} CSVs verschoben/extrahiert.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Cleanup bei Fehler: Versuche, das temporäre Verzeichnis zu löschen\n",
    "        if os.path.exists(temp_extract_dir):\n",
    "             shutil.rmtree(temp_extract_dir)\n",
    "             \n",
    "        return f\"FEHLER: {zip_filename} konnte nicht verarbeitet werden. {e}\"\n",
    "\n",
    "def process_and_save_csv(csv_path):\n",
    "    \"\"\"Liest eine CSV, bereinigt Spaltennamen und speichert als temporäres Parquet.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Spaltennamen-Mapping erstellen (um alte und neue Formate zu vereinheitlichen)\n",
    "        df_temp = pd.read_csv(csv_path, nrows=0) # Nur Header lesen\n",
    "        df_cols = df_temp.columns.tolist()\n",
    "        \n",
    "        # Erzeuge ein Mapping, um alte Spaltennamen auf neue zu vereinheitlichen\n",
    "        column_map = {}\n",
    "        use_cols_final = []\n",
    "\n",
    "        # Nutzerspalte usertype/member_casual ist in verschiedenen Dateien unterschiedlich kodiert\n",
    "        requires_usertype_conversion = False # Flag für die Wertekonvertierung\n",
    "        \n",
    "        for col in df_cols:\n",
    "            # --- Datum/Zeit-Daten standardisieren ---\n",
    "            if 'start_time' in col or 'started_at' in col or 'starttime' in col or 'Start Time' in col:\n",
    "                column_map[col] = 'started_at'\n",
    "                use_cols_final.append('started_at')\n",
    "            elif 'stop_time' in col or 'ended_at' in col or 'stoptime' in col or 'Stop Time' in col:\n",
    "                column_map[col] = 'ended_at'\n",
    "                use_cols_final.append('ended_at')\n",
    "            \n",
    "            # --- Geo-Daten standardisieren ---\n",
    "            elif 'start station latitude' in col or 'start_lat' in col or 'Start Station Latitude' in col:\n",
    "                column_map[col] = 'start_lat'\n",
    "                use_cols_final.append('start_lat')\n",
    "            elif 'end station latitude' in col or 'end_lat' in col or 'End Station Latitude' in col:\n",
    "                column_map[col] = 'end_lat'\n",
    "                use_cols_final.append('end_lat')\n",
    "            elif 'start station longitude' in col or 'start_lng' in col or 'Start Station Longitude' in col:\n",
    "                column_map[col] = 'start_lng'\n",
    "                use_cols_final.append('start_lng')    \n",
    "            elif 'end station longitude' in col or 'end_lng' in col or 'End Station Longitude' in col:\n",
    "                column_map[col] = 'end_lng'\n",
    "                use_cols_final.append('end_lng')    \n",
    "            \n",
    "            # --- Stations Namen/ID standardisieren ---\n",
    "            elif 'start station name' in col or 'start_station_name' in col or 'Start Station Name' in col:\n",
    "                column_map[col] = 'start_station_name'\n",
    "                use_cols_final.append('start_station_name')    \n",
    "            elif 'end station name' in col or 'end_station_name' in col or 'End Station Name' in col:\n",
    "                column_map[col] = 'end_station_name'\n",
    "                use_cols_final.append('end_station_name')      \n",
    "            elif 'start station id' in col or 'start_station_id' in col or 'Start Station ID' in col:\n",
    "                column_map[col] = 'start_station_id'\n",
    "                use_cols_final.append('start_station_id')    \n",
    "            elif 'end station id' in col or 'end_station_id' in col or 'End Station ID' in col:\n",
    "                column_map[col] = 'end_station_id'\n",
    "                use_cols_final.append('end_station_id')    \n",
    "            \n",
    "            # --- Rideable/Bike Typ standardisieren ---\n",
    "            elif 'rideable_type' in col:\n",
    "                column_map[col] = 'rideable_type'\n",
    "                use_cols_final.append('rideable_type') \n",
    "            \n",
    "            # --- Nutzerspalte standardisieren ---\n",
    "            elif 'member_casual' in col:\n",
    "                column_map[col] = 'member_casual'\n",
    "                use_cols_final.append('member_casual')\n",
    "            elif 'usertype' in col or 'User Type' in col:\n",
    "                column_map[col] = 'member_casual'\n",
    "                use_cols_final.append('member_casual')\n",
    "                requires_usertype_conversion = True\n",
    "            else:\n",
    "                print(f\"Unbekannte Spalte gefunden: {col}. Bitte erweitern Sie das Mapping.\")\n",
    "        \n",
    "        # Entferne Duplikate aus use_cols_final\n",
    "        use_cols_final = list(set(use_cols_final))\n",
    "        \n",
    "        # Datei einlesen\n",
    "        df = pd.read_csv(csv_path, \n",
    "                            usecols=list(column_map.keys()), \n",
    "                            dtype=DTYPE_MAP,\n",
    "                            low_memory=False)\n",
    "\n",
    "        # 2019 wurden bei Citibike die E-Bikes eingeführt. Davor gab es nur klassische Bikes. Daher fügen wir diese Spalte bei alten Daten hinzu.\n",
    "        if 'rideable_type' not in df.columns:\n",
    "            df['rideable_type'] = 'classic_bike'\n",
    "\n",
    "\n",
    "        # Spalten umbenennen \n",
    "        df.rename(columns=column_map, inplace=True)\n",
    "        \n",
    "        # Anpassung der Nutzerspalte falls erforderlich\n",
    "        if requires_usertype_conversion:\n",
    "                    df['member_casual'] = df['member_casual'].astype('object').replace({'Subscriber': 'member', 'Customer': 'casual'})\n",
    "\n",
    "        df['member_casual'] = df['member_casual'].astype('category')\n",
    "        # Nur die vereinheitlichten Spalten beibehalten\n",
    "        #   Einträge die keinen Mehrwert bieten oder die nur in alten Daten existieren  wurden nicht zu use_cols_final hinzugefügt\n",
    "        #   2014-2019: tripduration,bikeid,gender,birth year\n",
    "        #   2020-2025: ride_id\n",
    "        \n",
    "        # Diese Spalten liegen teils als rein numerische und teils als alpha-numerische Daten vor und müssen konsistent als String vorliegen\n",
    "        id_cols_to_fix = ['start_station_id', 'end_station_id']\n",
    "        for col in id_cols_to_fix:\n",
    "            if col in df.columns:\n",
    "                # 1.Alles in Strings umwandeln (auch NaNs werden zu 'nan')\n",
    "                df[col] = df[col].astype(str)\n",
    "                                \n",
    "                # 2. 'nan' Strings (von echten NaNs) wieder zu echten Python-None machen\n",
    "                #    Das erlaubt PyArrow, sie als NULL-Werte im String-Schema zu speichern\n",
    "                df[col] = df[col].replace(['nan', 'NaN', 'None', ''], None)\n",
    "        \n",
    "        df = df[use_cols_final]\n",
    "\n",
    "        # Speichern als temporäres Parquet\n",
    "        temp_parquet_path = os.path.join(PROCESSED_DATA_DIR, f'temp_{os.path.basename(csv_path)}.parquet')\n",
    "        df.to_parquet(temp_parquet_path, index=False)\n",
    "\n",
    "        \n",
    "        # Speicher freigeben\n",
    "        del df\n",
    "        return f\"Erfolg: {os.path.basename(csv_path)} verarbeitet und als Parquet gespeichert.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"FEHLER beim Verarbeiten: {os.path.basename(csv_path)}. {e}\"\n",
    "    \n",
    "def orchestrate_data_pipeline():\n",
    "    \n",
    "    # 1. Ordner erstellen\n",
    "    os.makedirs(UNZIPPED_DIR, exist_ok=True)\n",
    "    os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # 2. Alle heruntergeladenen ZIP-Dateien finden\n",
    "    all_zip_files = glob.glob(os.path.join(RAW_DATA_DIR, '*.zip'))\n",
    "\n",
    "    if not all_zip_files:\n",
    "        print(f\"KRITISCHER FEHLER: Keine ZIP-Dateien im Ordner {RAW_DATA_DIR} gefunden.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Starte Verarbeitung von {len(all_zip_files)} ZIP-Dateien...\")\n",
    "        \n",
    "    # 3. Alle ZIPs parallel entpacken\n",
    "    print(\"\\n--- Phase 1: Entpacken der ZIPs (parallel) ---\")\n",
    "    # Wir filtern die 'Überspringen'-Meldungen in der tqdm-Schleife für eine saubere Ausgabe.\n",
    "    skipped_count = 0\n",
    "    # ThreadPoolExecutor anstatt ProcessPoolExecutor (macht Probleme) verwenden\n",
    "    # with ThreadPoolExecutor(max_workers=32) as executor: \n",
    "    #     futures = [executor.submit(unzip_file, zp, UNZIPPED_DIR) for zp in all_zip_files]\n",
    "    #     for future in tqdm(as_completed(futures), total=len(all_zip_files), desc=\"Entpacke\"):\n",
    "    #         result = future.result()\n",
    "    #         if \"FEHLER\" in result:\n",
    "    #             print(f\"\\n{result}\")\n",
    "    #         elif \"Überspringe\" in result:\n",
    "    #             skipped_count += 1\n",
    "\n",
    "    # TEST: Sequenzielle Schleife nutzen, falls Multithreading/-processing Probleme macht:\n",
    "    # print(\"\\n--- Phase 1: Sequenzieller Testlauf Entpacken ---\")\n",
    "    # skipped_count = 0\n",
    "    # for zip_path in tqdm(all_zip_files, desc=\"Entpacke Sequenziell\"):\n",
    "    #     result = unzip_file(zip_path, UNZIPPED_DIR)\n",
    "    #     if \"FEHLER\" in result:\n",
    "    #         print(f\"\\n{result}\")\n",
    "    #         # Beenden Sie hier, um den Fehler zu sehen\n",
    "    #         raise Exception(\"Sequenzieller Entpack-Fehler aufgetreten.\")\n",
    "    #     elif \"Überspringe\" in result:\n",
    "    #         skipped_count += 1\n",
    "\n",
    "    if skipped_count > 0:\n",
    "        print(f\"INFO: {skipped_count} ZIP-Dateien wurden übersprungen, da die entpackten Daten bereits existieren.\")\n",
    "\n",
    "    # 4. Alle entpackten CSVs finden\n",
    "    all_csv_files = glob.glob(os.path.join(UNZIPPED_DIR, '*.csv'))\n",
    "    print(f\"\\n{len(all_csv_files)} CSV-Dateien gefunden und bereit zur Parallelverarbeitung.\")\n",
    "    \n",
    "    # 5. CSVs verarbeiten und als temporäre Parquet-Files speichern\n",
    "\n",
    "    # Parallel verarbeiten macht Probleme \n",
    "    # print(\"\\n--- Phase 2: Parallelverarbeitung der CSVs ---\")\n",
    "    # with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    #     futures = [executor.submit(process_and_save_csv, csvp) for csvp in all_csv_files]\n",
    "    #     for future in tqdm(as_completed(futures), total=len(all_csv_files), desc=\"Verarbeite CSVs\"):\n",
    "    #         result = future.result()\n",
    "    #         if \"FEHLER\" in result:\n",
    "    #             print(f\"\\n{result}\")\n",
    "    \n",
    "\n",
    "    # Sequenzielle Schleife nutzen, da Multiprocessing Probleme macht (auskommentiert um diesen Prozess nur einmal zu durchlaufen):\n",
    "    print(\"\\n--- Phase 2: Sequenzielle Verarbeitung der CSVs ---\")\n",
    "    # for csv_path in tqdm(all_csv_files, desc=\"Verarbeite CSVs Sequenziell\"):\n",
    "    #     result = process_and_save_csv(csv_path)\n",
    "    #     if \"FEHLER\" in result:\n",
    "    #         print(f\"\\n{result}\")\n",
    "    #         # Wenn hier ein Fehler auftritt, ist es ein Daten-/Code-Fehler, kein ProcessPool-Problem!\n",
    "    #         raise Exception(\"Sequenzieller Verarbeitungsfehler aufgetreten.\")            \n",
    "    \n",
    "    # 6. Konsolidierung & Speichern\n",
    "    print(\"\\n--- Phase 3: Finale Konsolidierung ---\")\n",
    "    final_parquet_files = glob.glob(os.path.join(PROCESSED_DATA_DIR, 'temp_*.parquet'))\n",
    "    print(f\"Füge {len(final_parquet_files)} temporäre Parquet-Dateien zusammen...\")\n",
    "    \n",
    "    # Verwende Pandas.Concat für speichereffiziente Konsolidierung\n",
    "    # df_final = pd.concat([pd.read_parquet(f) for f in tqdm(final_parquet_files, desc=\"Lade Parquet Chunks\")], \n",
    "    #                     ignore_index=True)\n",
    "    # df_final.to_parquet(FINAL_PARQUET_PATH, index=False)\n",
    "\n",
    "    # Verwende PyArrow für speichereffiziente Konsolidierung, um Memory Overflow zu vermeiden\n",
    "    # --> Immernoch zu hohe Speicherauslastung --> Dask Parquet \n",
    "    # for f in tqdm(final_parquet_files, desc=\"Lade Parquet Chunks\"):\n",
    "    #     table = pq.read_table(f)\n",
    "    #     tables.append(table)\n",
    "    # # Kombiniere alle geladenen Tabellen\n",
    "    # combined_table = pa.concat_tables(tables)\n",
    "    # # Schreibe die kombinierte Tabelle in die Zieldatei\n",
    "    # pq.write_table(combined_table, FINAL_PARQUET_PATH)\n",
    "\n",
    "    # Verwende Dask Parquet für speichereffiziente Konsolidierung, um Memory Overflow zu vermeiden\n",
    "    print(f\"Lese {len(final_parquet_files)} Parquet-Dateien (lazy)...\")\n",
    "    ddf = dd.read_parquet(final_parquet_files)\n",
    "    print(f\"Schreibe die kombinierte Tabelle nach {FINAL_PARQUET_PATH}...\")\n",
    "    ddf.to_parquet(FINAL_PARQUET_PATH, write_index=False)\n",
    " \n",
    "    print(f\"\\n--- ERFOLG ---\")\n",
    "    print(f\"Gesamtdatensatz gespeichert unter: {FINAL_PARQUET_PATH}\")\n",
    "           \n",
    "    # 7. Aufräumen (optional)\n",
    "    # Entfernen Sie die temporären Dateien, um Speicherplatz zu sparen\n",
    "    # for f in final_parquet_files: os.remove(f)\n",
    "    # for f in all_csv_files: os.remove(f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Achtung: Die Ausführung dieses Skripts kann bei allen Jahren sehr lange dauern!\n",
    "    orchestrate_data_pipeline()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ddb89",
   "metadata": {},
   "source": [
    "NYPD DATEIEN - VERARBEITEN\n",
    "1. CSV einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37877630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Einlesen Datenbank 1: NYPD Motor Vehicle Collisions\n",
    "\n",
    "NYPD_IN_FILE = 'Motor_Vehicle_Collisions_-_Crashes_20251209.csv'\n",
    "nypd_path = os.path.join('../data/unzipped/nypd/', NYPD_IN_FILE)\n",
    "\n",
    "print(f\"Lese NYPD-Daten ein von: {nypd_path}\")\n",
    "\n",
    "try:\n",
    "    # Nur die relevantesten Spalten zur Optimierung der Ladezeit auswählen\n",
    "    nypd_cols = [\"CRASH DATE\",\"CRASH TIME\",\"BOROUGH\",\"ZIP CODE\",\"LATITUDE\",\"LONGITUDE\",\"LOCATION\",\"ON STREET NAME\",\"CROSS STREET NAME\",\"OFF STREET NAME\",\"NUMBER OF PERSONS INJURED\",\"NUMBER OF PERSONS KILLED\",\"NUMBER OF PEDESTRIANS INJURED\",\"NUMBER OF PEDESTRIANS KILLED\",\"NUMBER OF CYCLIST INJURED\",\"NUMBER OF CYCLIST KILLED\",\"NUMBER OF MOTORIST INJURED\",\"NUMBER OF MOTORIST KILLED\",\"CONTRIBUTING FACTOR VEHICLE 1\",\"CONTRIBUTING FACTOR VEHICLE 2\",\"CONTRIBUTING FACTOR VEHICLE 3\",\"CONTRIBUTING FACTOR VEHICLE 4\",\"CONTRIBUTING FACTOR VEHICLE 5\",\"COLLISION_ID\",\"VEHICLE TYPE CODE 1\",\"VEHICLE TYPE CODE 2\",\"VEHICLE TYPE CODE 3\",\"VEHICLE TYPE CODE 4\",\"VEHICLE TYPE CODE 5\"]\n",
    "    #   nypd_cols = ['CRASH DATE', 'CRASH TIME', 'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF CYCLIST INJURED','VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']\n",
    "\n",
    "    df_nypd = pd.read_csv(nypd_path, usecols=lambda x: x in nypd_cols, encoding='latin1', low_memory=False)\n",
    "#     ddf_nypd = dd.read_csv(\n",
    "#     nypd_path, \n",
    "#     usecols=nypd_cols,    # Dask akzeptiert die Liste nypd_cols direkt\n",
    "#     encoding='latin1', \n",
    "#     # low_memory=False,   # In Dask nicht nötig/vorhanden, da Dask sowieso chunkweise arbeitet\n",
    "#     dtype='object'         # Empfehlung: Erstmal als object laden, um Typ-Konflikte zu vermeiden\n",
    "# )\n",
    "    print(f\"\\n=======================================================\")\n",
    "    print(f\"\\nNYPD-Daten erfolgreich geladen. {len(df_nypd):,} Zeilen.\")\n",
    "    print(f\"=======================================================\")\n",
    "    print(df_nypd.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"FEHLER: Datei {NYPD_IN_FILE} nicht im angegebenen Pfad gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e82bb",
   "metadata": {},
   "source": [
    "2. CSV Dateien bereinigen und als Parquet speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79665edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bereinigen Datenbank 1: NYPD Motor Vehicle Collisions\n",
    "os.makedirs('../data/processed/nypd', exist_ok=True)\n",
    "print(f\"NYPD-Daten (unbereinigt): {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "# Koordinaten filtern (entfernt Zeilen ohne Geo-Daten)\n",
    "df_nypd.dropna(subset=['LATITUDE', 'LONGITUDE'], inplace=True)\n",
    "print(f\"NYPD-Daten nach Geo-Bereinigung: {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "# Crash Date/Time in Datetime-Typ konvertieren; Die Fehler-Toleranz (errors='coerce') setzt ungültige Daten auf NaT (Not a Time)\n",
    "df_nypd['CRASH DATE'] = pd.to_datetime(df_nypd['CRASH DATE'], errors='coerce')\n",
    "\n",
    "# Prüfen, ob die 'CRASH TIME'-Spalte bereits datetime.time-Objekte enthält.\n",
    "# Dies verhindert, dass pd.to_datetime bei bereits konvertierten Objekten fehlschlägt.\n",
    "if not df_nypd['CRASH TIME'].dropna().empty and \\\n",
    "   isinstance(df_nypd['CRASH TIME'].dropna().iloc[0], dt.time):\n",
    "    pass\n",
    "else:\n",
    "    df_nypd['CRASH TIME'] = pd.to_datetime(df_nypd['CRASH TIME'], format='%H:%M', errors='coerce').dt.time\n",
    "\n",
    "\n",
    "# Die CRASH TIME liegt als Python 'time' Objekt vor, das nicht direkt addiert werden kann.\n",
    "def time_to_seconds(t):\n",
    "    \"\"\"Konvertiert ein datetime.time Objekt in die Gesamtanzahl der Sekunden.\"\"\"\n",
    "    # fängt fehlerhafte time Objekte ab\n",
    "    if pd.isna(t):\n",
    "        return pd.NA\n",
    "    return t.hour * 3600 + t.minute * 60 + t.second\n",
    "\n",
    "# Konvertiere in Sekunden, bilde ein Zeitdelta und addiere dieses zum Datum\n",
    "df_nypd['time_seconds'] = df_nypd['CRASH TIME'].apply(time_to_seconds)\n",
    "df_nypd['time_delta'] = pd.to_timedelta(df_nypd['time_seconds'], unit='s')\n",
    "df_nypd['crash_datetime'] = df_nypd['CRASH DATE'] + df_nypd['time_delta']\n",
    "\n",
    "# Entferne die temporären Spalten und Zeilen, die nach der Kombination ungültig sind.\n",
    "df_nypd.drop(columns=['time_seconds', 'time_delta'], inplace=True)\n",
    "df_nypd.dropna(subset=['crash_datetime'], inplace=True)\n",
    "print(f\"NYPD-Daten nach Datetime-Konvertierung und -Bereinigung: {len(df_nypd):,} Zeilen.\")\n",
    "\n",
    "\n",
    "# Bereinigung: Unfallbeteiligte filtern (entfernt Zeilen ohne  Fahrradbeteiligung)\n",
    "# -- Fall A: Prüfen, ob \"bike\" / \"bicycle\" in den Fahrzeugtyp-Codes vorkommt\n",
    "BIKE_IDENTIFIER = \"bike|bicycle\"\n",
    "\n",
    "bicycle_in_codes = (\n",
    "    df_nypd['VEHICLE TYPE CODE 1'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 2'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 3'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 4'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False) |\n",
    "    df_nypd['VEHICLE TYPE CODE 5'].astype(str).str.contains(BIKE_IDENTIFIER, case=False, na=False)\n",
    ")\n",
    "\n",
    "# -- Fall B: Prüfen, ob eine Person als Radfahrer verletzt oder getötet wurde\n",
    "# (Stellt sicher, dass auch nicht explizit als \"bike\" / \"bicycle\"  gekennzeichnete, aber involvierte Fahrräder erfasst werden)\n",
    "cyclist_injured = df_nypd['NUMBER OF CYCLIST INJURED'].fillna(0).astype(int) > 0\n",
    "cyclist_killed = df_nypd['NUMBER OF CYCLIST KILLED'].fillna(0).astype(int) > 0\n",
    "\n",
    "# Kombiniere die beiden Fälle (OR-Verknüpfung)\n",
    "df_nypd_filtered = df_nypd[bicycle_in_codes | cyclist_injured | cyclist_killed].copy()\n",
    "print(f\"\\nNYPD-Daten nach Filterung auf Fahrrad-Unfälle: {len(df_nypd_filtered):,} Zeilen.\")\n",
    "# Füge eine Spalte hinzu, die anzeigt, dass Fahrräder beteiligt sind\n",
    "df_nypd_filtered['crashes_with_bicycles_involved'] = 1\n",
    "# Reduziere den DataFrame auf die wesentlichen Geo- und Zeit-Spalten für die Analyse\n",
    "df_nypd_clean = df_nypd_filtered[[\n",
    "    'crash_datetime',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'NUMBER OF CYCLIST INJURED',\n",
    "    'NUMBER OF CYCLIST KILLED',\n",
    "    'VEHICLE TYPE CODE 1',\n",
    "    'VEHICLE TYPE CODE 2',\n",
    "    'VEHICLE TYPE CODE 3',\n",
    "    'VEHICLE TYPE CODE 4', \n",
    "    'VEHICLE TYPE CODE 5',\n",
    "    'crashes_with_bicycles_involved'\n",
    "]].copy()\n",
    "\n",
    "nypd_parquet_path = os.path.join('../data/processed/nypd', 'nypd_clean.parquet')\n",
    "df_nypd_clean.to_parquet(nypd_parquet_path, index=False)\n",
    "\n",
    "print(f\"\\n=======================================================\")\n",
    "print(f\"NYPD-Daten erfolgreich bereinigt. {len(df_nypd_clean):,} Zeilen.\")\n",
    "print(f\"=======================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axa-citibike-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
